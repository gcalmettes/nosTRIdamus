{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Races data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_races = pd.read_csv('./../data/clean/races_features.csv', index_col='race')\n",
    "len(df_races)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single results: 2000403\n",
      "Number of individuals: 643055\n",
      "Number of remaining single results: 945102\n",
      "Number of remaining individuals: 399472\n",
      "Number of races in results df: 156\n"
     ]
    }
   ],
   "source": [
    "from utils.get_processed_data import get_results_df\n",
    "\n",
    "df_results = get_results_df(df_races)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter results dataset to keep only prolific racers\n",
    "#### To prevent cold-start problem keep only people with at least several different races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.get_processed_data import get_athletes_races_count\n",
    "\n",
    "athlete_habits = get_athletes_races_count(df_results)\n",
    "athlete_habits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid individuals: 45799\n",
      "Number of remaining single results: 329329\n",
      "Number of races present in filtered results: 156\n"
     ]
    }
   ],
   "source": [
    "# mininum of different races we want our athletes to have done\n",
    "min_unique_race_count = 4\n",
    "\n",
    "# What we are filtering with\n",
    "max_count_races = 100 # probably similar names\n",
    "\n",
    "\n",
    "valid_athletes = athlete_habits.loc[\n",
    "    (athlete_habits['n_different_races']>=min_unique_race_count) \n",
    "    & (athlete_habits['n_races']<=max_count_races)\n",
    "]\n",
    "\n",
    "# use this df to filter original results data\n",
    "df_results_filtered = df_results.loc[df_results['athlete'].isin(valid_athletes['athlete'])]\n",
    "df_results_filtered = df_results_filtered.merge(valid_athletes, left_on=\"athlete\", right_on=\"athlete\", how=\"left\")\n",
    "\n",
    "# Anonimize entrants\n",
    "user_hash = {}\n",
    "\n",
    "for i,user in enumerate(df_results_filtered.athlete.unique()):\n",
    "    user_hash[user] = f'u{i}'\n",
    "df_results_filtered.loc[:, 'athlete'] = df_results_filtered.athlete.map(lambda x: user_hash[x])\n",
    "\n",
    "print(\"Number of valid individuals:\", len(valid_athletes))\n",
    "print(\"Number of remaining single results:\", len(df_results_filtered))\n",
    "print(\"Number of races present in filtered results:\", len(df_results_filtered.race.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update races df to only keep races present in filtered results df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining races: 156\n"
     ]
    }
   ],
   "source": [
    "df_races_for_model = df_races.loc[df_results_filtered.race.unique()]\n",
    "print(\"Number of remaining races:\", len(df_races_for_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe with race count per athlete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete</th>\n",
       "      <th>race</th>\n",
       "      <th>count</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>cozumel</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0</td>\n",
       "      <td>liuzhou70.3</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0</td>\n",
       "      <td>mardelplata</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0</td>\n",
       "      <td>newzealand</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u0</td>\n",
       "      <td>xiamen70.3</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  athlete         race  count gender country\n",
       "0      u0      cozumel      1      M     USA\n",
       "1      u0  liuzhou70.3      1      M     USA\n",
       "2      u0  mardelplata      1      M     USA\n",
       "3      u0   newzealand      1      M     USA\n",
       "4      u0   xiamen70.3      2      M     USA"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_races_count = (\n",
    "    df_results_filtered\n",
    "        .groupby(['athlete', 'race'])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: 'count'})\n",
    ")\n",
    "\n",
    "# Add demographics info for users (if we want to filter later)\n",
    "gender = (df_results_filtered\n",
    "     .groupby(['athlete', 'gender'])\n",
    "     .size()\n",
    "     .reset_index()\n",
    "     .rename(columns={0: 'n'})\n",
    "     .pivot(index='athlete', columns='gender', values='n')\n",
    "     .idxmax(axis=1)\n",
    "     .rename('gender')\n",
    ")\n",
    "\n",
    "country = (df_results_filtered\n",
    "     .groupby(['athlete', 'country'])\n",
    "     .size()\n",
    "     .reset_index()\n",
    "     .rename(columns={0: 'n'})\n",
    "     .pivot(index='athlete', columns='country', values='n')\n",
    "     .idxmax(axis=1)\n",
    "     .rename('country')\n",
    ")\n",
    "\n",
    "results_races_count = results_races_count.merge(gender, left_on = 'athlete', right_on = 'athlete', how = 'left')\n",
    "results_races_count = results_races_count.merge(country, left_on = 'athlete', right_on = 'athlete', how = 'left')\n",
    "\n",
    "results_races_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender:\n",
    "    def __init__(self, model, matrix, items_info, name='Model'):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.matrix = matrix\n",
    "        self.items_info = items_info\n",
    "        self.items_info_reset = items_info.reset_index()\n",
    "        \n",
    "\n",
    "class ALSRecommender(BaseRecommender):\n",
    "     def recommend(self, target, n=10, filterByField=False, valueToMatch=False):\n",
    "        target_code = self.items_info.index.get_loc(target)\n",
    "        similar = self.model.similar_items(target_code, len(self.items_info))\n",
    "        \n",
    "        df_distances = pd.DataFrame([\n",
    "                [self.items_info_reset.loc[code, 'race'], distance] for (code,distance) in similar\n",
    "            ], columns=['race', 'distance'])\n",
    "        \n",
    "        df_order = df_distances.merge(self.items_info, left_on='race', right_on='race', how='left')\n",
    "        if filterByField:\n",
    "            df_order = df_order.loc[df_order[filterByField] == valueToMatch]\n",
    "\n",
    "        return df_order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Least Square (ALS)\n",
    "\n",
    "The data we have are implicit data (data gathered from the users behaviour, with no ratings or specific actions needed. It could be what items a user purchased, how many times they played a song or watched a movie, how long they’ve spent reading a specific article etc. The upside is that we have a lot more of this data, the downside is that it’s more noisy and not always apparent what it means.)\n",
    "\n",
    "ALS has been developped specifically for implicit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: (156, 45799)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "full_set = results_races_count.copy()\n",
    "full_set_pivot = full_set.pivot(index='race', columns='athlete', values='count').fillna(0)\n",
    "sparse_item_user = sparse.csr_matrix(full_set_pivot.values)\n",
    "\n",
    "print(\"Matrix size:\", sparse_item_user.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparcity of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9639170265166466"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_size = sparse_item_user.shape[0]*sparse_item_user.shape[1] # Number of possible interactions in the matrix\n",
    "num_raced = len(sparse_item_user.nonzero()[0]) # Number of items interacted with\n",
    "sparsity = (1 - (num_raced/matrix_size))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use local implicit library (has been compiled using latest github code)\n",
    "from implicit_local.evaluation import train_test_split, mean_average_precision_at_k, ndcg_at_k\n",
    "from implicit_local.als import AlternatingLeastSquares\n",
    "from implicit_local.nearest_neighbours import CosineRecommender\n",
    "\n",
    "train, test = train_test_split(sparse_item_user, train_percentage=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change that if you want to re-run the Grid search\n",
    "runTheGridSearch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 combinations of hyperparameters will be tested\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'confidence_factor': [1, 5, 10, 20, 30, 40], \n",
    "    'als__factors': [5, 8, 10, 13, 17, 21, 25, 30],\n",
    "    'als__regularization': [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "params_combinations = list(ParameterGrid(param_grid))\n",
    "print(f'{len(params_combinations)} combinations of hyperparameters will be tested')\n",
    "\n",
    "if runTheGridSearch:\n",
    "    als_evaluation = {}\n",
    "\n",
    "    for i,params in enumerate(params_combinations):\n",
    "        train_conf = (train * params['confidence_factor']).astype('double')\n",
    "        model = AlternatingLeastSquares(factors=params['als__factors'], regularization=params['als__regularization'], iterations=50)\n",
    "        model.fit(train_conf, show_progress=False)\n",
    "\n",
    "        map5 = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=5, num_threads=4, show_progress=False)\n",
    "        map10 = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4, show_progress=False)\n",
    "        ndcg5 = ndcg_at_k(model, train.T.tocsr(), test.T.tocsr(), K=5, num_threads=4, show_progress=False)\n",
    "        ndcg10 = ndcg_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4, show_progress=False)\n",
    "\n",
    "        als_evaluation[i] = {\n",
    "            'map@5': map5,\n",
    "            'map@10': map10,\n",
    "            'ndcg@5': ndcg5,\n",
    "            'ndcg@10': ndcg10   \n",
    "        } \n",
    "\n",
    "    evaluation_results = pd.DataFrame([\n",
    "        [params_combinations[i]['confidence_factor'], \n",
    "        params_combinations[i]['als__factors'],\n",
    "        params_combinations[i]['als__regularization'], \n",
    "        als_evaluation[i]['map@5'], \n",
    "        als_evaluation[i]['map@10'],\n",
    "        als_evaluation[i]['ndcg@5'], \n",
    "        als_evaluation[i]['ndcg@10']\n",
    "        ] \n",
    "        for i in range(len(params_combinations))\n",
    "    ], columns=['confidence', 'n_factors', 'regularization', 'map@5', 'map@10', 'ndcg@5', 'ndcg@10'])\n",
    "\n",
    "    evaluation_results.to_csv('validation-results/als_hyperparameters_tuning.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    evaluation_results = pd.read_csv('validation-results/als_hyperparameters_tuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>n_factors</th>\n",
       "      <th>regularization</th>\n",
       "      <th>map@5</th>\n",
       "      <th>map@10</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.229828</td>\n",
       "      <td>0.257501</td>\n",
       "      <td>0.292178</td>\n",
       "      <td>0.350453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     confidence  n_factors  regularization     map@5    map@10    ndcg@5  \\\n",
       "103           5         17            0.01  0.229828  0.257501  0.292178   \n",
       "\n",
       "      ndcg@10  \n",
       "103  0.350453  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results.loc[evaluation_results['map@5'] == evaluation_results['map@5'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>n_factors</th>\n",
       "      <th>regularization</th>\n",
       "      <th>map@5</th>\n",
       "      <th>map@10</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.229828</td>\n",
       "      <td>0.257501</td>\n",
       "      <td>0.292178</td>\n",
       "      <td>0.350453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     confidence  n_factors  regularization     map@5    map@10    ndcg@5  \\\n",
       "103           5         17            0.01  0.229828  0.257501  0.292178   \n",
       "\n",
       "      ndcg@10  \n",
       "103  0.350453  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results.loc[evaluation_results['ndcg@5'] == evaluation_results['ndcg@5'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's refine\n",
    "\n",
    "if runTheGridSearch:\n",
    "    best_params = evaluation_results.loc[evaluation_results['map@5'] == evaluation_results['map@5'].max()]\n",
    "\n",
    "    param_grid_refined = {\n",
    "        'confidence_factor': np.arange(best_params['confidence'].values[0]-2, best_params['confidence'].values[0]+3.5, 0.5), \n",
    "        'als__factors': np.arange(best_params['n_factors'].values[0]-3, best_params['n_factors'].values[0]+4, 1),\n",
    "        'als__regularization': [1e-2]\n",
    "    }\n",
    "\n",
    "    params_combinations_refined = list(ParameterGrid(param_grid_refined))\n",
    "    print(f'{len(params_combinations_refined)} combinations of hyperparameters will be tested')\n",
    "\n",
    "\n",
    "    als_evaluation_refined = {}\n",
    "\n",
    "    for i,params in enumerate(params_combinations_refined):\n",
    "        train_conf = (train * params['confidence_factor']).astype('double')\n",
    "        model = AlternatingLeastSquares(factors=params['als__factors'], regularization=params['als__regularization'], iterations=50)\n",
    "        model.fit(train_conf, show_progress=False)\n",
    "\n",
    "        map5 = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=5, num_threads=4, show_progress=False)\n",
    "        map10 = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4, show_progress=False)\n",
    "        ndcg5 = ndcg_at_k(model, train.T.tocsr(), test.T.tocsr(), K=5, num_threads=4, show_progress=False)\n",
    "        ndcg10 = ndcg_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4, show_progress=False)\n",
    "\n",
    "        als_evaluation_refined[i] = {\n",
    "            'map@5': map5,\n",
    "            'map@10': map10,\n",
    "            'ndcg@5': ndcg5,\n",
    "            'ndcg@10': ndcg10   \n",
    "        } \n",
    "\n",
    "    evaluation_results_refined = pd.DataFrame([\n",
    "        [params_combinations_refined[i]['confidence_factor'], \n",
    "        params_combinations_refined[i]['als__factors'],\n",
    "        params_combinations_refined[i]['als__regularization'], \n",
    "        als_evaluation_refined[i]['map@5'], \n",
    "        als_evaluation_refined[i]['map@10'],\n",
    "        als_evaluation_refined[i]['ndcg@5'], \n",
    "        als_evaluation_refined[i]['ndcg@10']\n",
    "        ] \n",
    "        for i in range(len(params_combinations_refined))\n",
    "    ], columns=['confidence', 'n_factors', 'regularization', 'map@5', 'map@10', 'ndcg@5', 'ndcg@10'])\n",
    "\n",
    "    evaluation_results_refined.to_csv('validation-results/als_hyperparameters_tuning-refined.csv', index=False)\n",
    "\n",
    "else:\n",
    "    evaluation_results_refined = pd.read_csv('validation-results/als_hyperparameters_tuning-refined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>n_factors</th>\n",
       "      <th>regularization</th>\n",
       "      <th>map@5</th>\n",
       "      <th>map@10</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.23045</td>\n",
       "      <td>0.257516</td>\n",
       "      <td>0.292607</td>\n",
       "      <td>0.349533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    confidence  n_factors  regularization    map@5    map@10    ndcg@5  \\\n",
       "48         5.0         18            0.01  0.23045  0.257516  0.292607   \n",
       "\n",
       "     ndcg@10  \n",
       "48  0.349533  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results_refined.loc[evaluation_results_refined['map@5'] == evaluation_results_refined['map@5'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_base = './../flask_app/nostrappdamus/model/data'\n",
    "\n",
    "# save the matrix to disk\n",
    "sparse.save_npz(f'{filename_base}/als_sparse_matrix.npz', sparse_item_user)\n",
    "\n",
    "# save order of races used\n",
    "with open(f'{filename_base}/als_hash.json', 'w') as f:\n",
    "    race_hash = df_races_for_model.loc[full_set_pivot.index].reset_index()['race'].to_dict()\n",
    "    f.write(json.dumps(race_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>n_factors</th>\n",
       "      <th>regularization</th>\n",
       "      <th>map@5</th>\n",
       "      <th>map@10</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.230450</td>\n",
       "      <td>0.257516</td>\n",
       "      <td>0.292607</td>\n",
       "      <td>0.349533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.229829</td>\n",
       "      <td>0.257877</td>\n",
       "      <td>0.291642</td>\n",
       "      <td>0.350520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.230450</td>\n",
       "      <td>0.257516</td>\n",
       "      <td>0.292607</td>\n",
       "      <td>0.349533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.229563</td>\n",
       "      <td>0.257642</td>\n",
       "      <td>0.291830</td>\n",
       "      <td>0.350947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    confidence  n_factors  regularization     map@5    map@10    ndcg@5  \\\n",
       "48         5.0         18            0.01  0.230450  0.257516  0.292607   \n",
       "25         4.5         16            0.01  0.229829  0.257877  0.291642   \n",
       "48         5.0         18            0.01  0.230450  0.257516  0.292607   \n",
       "28         6.0         16            0.01  0.229563  0.257642  0.291830   \n",
       "\n",
       "     ndcg@10  \n",
       "48  0.349533  \n",
       "25  0.350520  \n",
       "48  0.349533  \n",
       "28  0.350947  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_df = pd.concat([\n",
    "    evaluation_results_refined.loc[evaluation_results_refined[metric] == evaluation_results_refined[metric].max()]\n",
    "        for metric in ['map@5', 'map@10', 'ndcg@5', 'ndcg@10']\n",
    "])\n",
    "\n",
    "best_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence         5.000000\n",
       "n_factors         18.000000\n",
       "regularization     0.010000\n",
       "map@5              0.230450\n",
       "map@10             0.257516\n",
       "ndcg@5             0.292607\n",
       "ndcg@10            0.349533\n",
       "Name: 48, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# are they any indice that gives the best result for two or more of the metrics?\n",
    "best_idx = mode(best_params_df.index)[0][0]\n",
    "best_params = evaluation_results_refined.loc[best_idx]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45799/45799 [00:03<00:00, 12984.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4909675054755087"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to import global implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# fit the model on all the data using those params\n",
    "full_set_conf = (sparse_item_user * best_params['confidence']).astype('double')\n",
    "\n",
    "als_final = AlternatingLeastSquares(factors=int(best_params['n_factors']), regularization=best_params['regularization'], iterations=50)\n",
    "als_final.fit(full_set_conf, show_progress=False)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "pickle.dump(als_final, open(f\"{filename_base}/als_model.sav\", 'wb'))\n",
    "\n",
    "mean_average_precision_at_k(als_final, train.T.tocsr(), test.T.tocsr(), K=5, num_threads=4)\n",
    "\n",
    "# # some time later...\n",
    " \n",
    "# # load the model from disk\n",
    "# loaded_model = pickle.load(open(f\"{filename_base}/als-model.sav\", 'rb'))\n",
    "# loaded_matrix = sparse.load_npz(f\"{filename_base}/als_sparse_matrix.npz\")\n",
    "# with open(f\"{filename_base}/als_race_hash.json\", \"r\") as f:\n",
    "#     hash_code_to_race = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45799/45799 [00:01<00:00, 44277.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2710724630735964"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to Cosine Recommender\n",
    "cosine = CosineRecommender(K=50, num_threads=4)\n",
    "cosine.fit(sparse_item_user, show_progress=False)\n",
    "mean_average_precision_at_k(cosine, train.T.tocsr(), test.T.tocsr(), K=5, num_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarizing the race relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set_pivot_binarized = full_set_pivot.apply(np.sign)\n",
    "\n",
    "sparse_item_user_binarized = sparse.csr_matrix(full_set_pivot_binarized.values)\n",
    "\n",
    "train_binarized, test_binarized = train_test_split(sparse_item_user_binarized, train_percentage=0.8)\n",
    "\n",
    "if runTheGridSearch:\n",
    "\n",
    "    als_evaluation_binarized = {}\n",
    "\n",
    "\n",
    "    for i,params in enumerate(params_combinations):\n",
    "        train_conf = (train_binarized * params['confidence_factor']).astype('double')\n",
    "        model = AlternatingLeastSquares(factors=params['als__factors'], regularization=params['als__regularization'], iterations=50)\n",
    "        model.fit(train_conf, show_progress=False)\n",
    "\n",
    "        map5 = mean_average_precision_at_k(model, train_binarized.T.tocsr(), test_binarized.T.tocsr(), K=5, num_threads=4, show_progress=False)\n",
    "        map10 = mean_average_precision_at_k(model, train_binarized.T.tocsr(), test_binarized.T.tocsr(), K=10, num_threads=4, show_progress=False)\n",
    "        ndcg5 = ndcg_at_k(model, train_binarized.T.tocsr(), test_binarized.T.tocsr(), K=5, num_threads=4, show_progress=False)\n",
    "        ndcg10 = ndcg_at_k(model, train_binarized.T.tocsr(), test_binarized.T.tocsr(), K=10, num_threads=4, show_progress=False)\n",
    "\n",
    "        als_evaluation_binarized[i] = {\n",
    "            'map@5': map5,\n",
    "            'map@10': map10,\n",
    "            'ndcg@5': ndcg5,\n",
    "            'ndcg@10': ndcg10   \n",
    "        } \n",
    "    \n",
    "    evaluation_results_binarized = pd.DataFrame([\n",
    "        [params_combinations[i]['confidence_factor'], \n",
    "        params_combinations[i]['als__factors'],\n",
    "        params_combinations[i]['als__regularization'], \n",
    "        als_evaluation_binarized[i]['map@5'], \n",
    "        als_evaluation_binarized[i]['map@10'],\n",
    "        als_evaluation_binarized[i]['ndcg@5'], \n",
    "        als_evaluation_binarized[i]['ndcg@10']\n",
    "        ] \n",
    "        for i in range(len(params_combinations))\n",
    "    ], columns=['confidence', 'n_factors', 'regularization', 'map@5', 'map@10', 'ndcg@5', 'ndcg@10'])\n",
    "\n",
    "    evaluation_results_binarized.to_csv('validation-results/als_binarized_hyperparameters_tuning.csv', index=False)\n",
    "else:\n",
    "    evaluation_results_binarized = pd.read_csv('validation-results/als_binarized_hyperparameters_tuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>n_factors</th>\n",
       "      <th>regularization</th>\n",
       "      <th>map@5</th>\n",
       "      <th>map@10</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22544</td>\n",
       "      <td>0.252478</td>\n",
       "      <td>0.286371</td>\n",
       "      <td>0.344047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     confidence  n_factors  regularization    map@5    map@10    ndcg@5  \\\n",
       "103           5         17            0.01  0.22544  0.252478  0.286371   \n",
       "\n",
       "      ndcg@10  \n",
       "103  0.344047  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results_binarized.loc[evaluation_results['map@5'] == evaluation_results['map@5'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-science)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
